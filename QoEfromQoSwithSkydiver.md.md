---


---

<h2 id="overview">Overview:</h2>
<p>The objective of this PoC is to demonstrate the ability to estimate QoE, as perceived by the user, by applying cognitive methods to analyse network-level metrics collected by the service provider. The assumption is that the service provider can measure various QoS metrics; however, does not have full information on the actual QoE that the user is experiencing. Therefore, it must estimate the QoE from the measured QoS metrics. We employ machine learning (ML) to learn the QoS relationship to QoE through training with labelled examples. The learned data can be utilized at run-time to predict probable SLA violations and trigger corrective measures.</p>
<p>This approach is intended to be integrated into the SliceNet eHealth UC and exercises several SliceNet workflows; including, E2E service cognition, monitoring, and QoE feedback. The PoC is focused on the QoE KPI of E2E latency.<br>
Following the SliceNet data-driven operations approach, the model is deployed as part of the slice to generate an Estimated-QoE metric from monitored QoS KPIs. This metric is then consumed by slice control functions to trigger control and/or management actions required for proactively maintaining the service-level QoE, before any degradation affects the user.</p>
<p>This article emphasizes the role skydive plays in gathering QoS data and the transformations required to make its data useful input to the ML algorithm.</p>
<h2 id="poc-experiment-setup-and-implementation-details">PoC experiment setup and Implementation details:</h2>
<p>The PoC experiment setup is described in the Figure at <a href="https://drive.google.com/file/d/142FpFA_BjYh2hIZU3qlv67ibOoEdbxKe/view">https://drive.google.com/file/d/142FpFA_BjYh2hIZU3qlv67ibOoEdbxKe/view</a>.<br>
K8s Cluster 2 corresponds to a managed slice, where the slice provider collects QoS metrics (through SkyDive) in order to manage the slice QoE. Stresser nodes generate controlled traffic to vary the E2E service behaviour; namely the QoE of the client. Actual service-level E2E QoE metrics are collected (by test diver) and provided by the vertical (service user) for model training and validation. A ML classification process learns a QoE sensor model that estimates E2E QoE from measured QoS metrics. The model is then validated.</p>
<p>In order to simulate the user’s workload, an application to generate WordPress client traffic to our WordPress service has been implemented. A test driver was designed to coordinate the running of the WordPress client workloads, refered to as benchmark instances, concurrently with various levels of stress generated by iperf3 <a href="https://github.com/esnet/iperf">https://github.com/esnet/iperf</a>. The test driver also maintains an index that recorded the start time, end time, benchmark duration of each experimental sample used to map the benchmark duration (QoE) to the Skydive flows (QoS). The WordPress client bencmark consisted of multiple concurrent downloads of the two files, 5MB and 15MB, from WordPress Server. The downloads were done with HTTP Keep-Alive set to false. In the background the network stresser ran various levels of stress including no stress.<br>
Skydive collected network flow metrics (QoS). We directed Skydive to capture network flow metrics on the WordPress service interface, i.e. eth0.  The Test Driver’s Index and Skydive flows were copied to an IBM Cloud Object Store (COS).<br>
Analysis of the data was done in an IBM Watson Studio notebook with a Python 3.5 kernel. The Python Data Analysis Library (Pandas) was used to aggregate the QoS and QoE measures. Finally, the Python Scikit-learn library was used for creating the ML models.</p>
<h2 id="scenario">Scenario</h2>
<p>The PoC comes in two phases 1) Benchmark execution and data collection 2) ML analysis.</p>
<p><strong>Benchmark execution and data collection</strong>:</p>
<ol>
<li>Measuring QoE – in order to develop our approach, we created a controlled environment, where we can measure both network QoS parameters and application-level QoE, i.e. benchmark duration. We use a web service (WordPress) and measure the service level from the client perspective.  The application is created on two Kubernetes (K8s) container clouds deployed through the IBM ICP service; the wordpress client is on one cluster and the wordpress server is running on the other cluster.</li>
<li>Generating different quality of experiences – we generate “other” network traffic to the host on which the subject benchmark is running using iperf3 udp traffic. Multiple iperf3 servers are running on the same host as the wordpress server, while the iperf3 clients are launched on nodes other than the wordpress client.  The iperf3 clients are run concurrently with the wordpress benchmarks.  Various noise levels are generating by varying the number of client threads (-P), number of bytes transferred (-b), and  number running clients (one client per node).</li>
<li>Measuring QoS – under our simulation model assumptions, the slice service provider can only measure local metrics within its slice. In our environment, we limited the QoS measurements to the K8s cluster that runs the WordPress service; namely, there are no metrics from the client cluster. The QoS is derived from network flows captured by skydive on the interface belonging to the wordpress service. The skydive flow capture api is used to capture the flows in real time. The code collection code is a thread that runs concurrently with the benchmark instance.  The problem that it needs to be solved is to collect only the most recent flows for the current benchmark,  but flows from the previous benchmark is mixed in with fiows from the current benchmark and flows with the same UUID are being updated over time.  The tactic to solve this problem is to transforms the Skydive flows into pandas dataframes.  The gremlin expression indicates that it is only interested in tcp flows. It then gets residual flows from the previous benchmark.   In the loop it get new flows from the current benchmark and removes the residual flows from the previous benchmark. When the benchmark instance completes the test driver joins the Skydive thread which toggles the collectionFlows flag which in turn breaks the loop. After completing its loop the code concatenates all the flows that it collected, removes duplicates, and uses only the most recent flow identified by its UUID.</li>
<li>Labeling -   The QoE and QoS flows are labelled with the benchmark instance ID when they were captured. The labelled data is stored as csv files in IBM’s Cloud Object Store for ML analysis.</li>
</ol>
<p><strong>ML Analysis</strong></p>
<ol>
<li>Data Input - The ML analysis was done the IBM Watson Studio. The CSV files created in the first phase are read into a python 3.6 notebook.</li>
<li>Transformations - The labelled Skydive flows are transformed in pandas dataframe for use by the ML algorithms.  Addition per flow transformation  were required:</li>
</ol>
<ul>
<li>flow_duration:  Metric.Last - Metric.Start</li>
<li>bytes_per_flow: Metric.ABBytes + Metric.BABytes) / flow_duration</li>
<li>packets_per_flow: Metric.ABPackets + Metric.BAPackets / 'flow_duration</li>
<li>AB_bytes_per_flow:  Metric.ABBytes / flow_duration</li>
<li>BA_bytes_per_flow: Metric.BABytes  / flow_duration</li>
<li>AB_packets_per_flow: Metric.ABPackets / flow_duration</li>
<li>BA_packets_per_flow: Metric.BAPackets  / flow_duration</li>
<li>RTT:  Metric.RTT</li>
</ul>
<ol start="3">
<li>Aggregations:  The above transformations are then aggregated into per benchmark instance mean values as listed below:</li>
</ol>
<ul>
<li>flow_duration_mean</li>
<li>bytes_per_flow_mean</li>
<li>packets_per_flow_mean</li>
<li>AB_bytes_per_flow_mean</li>
<li>BA_bytes_per_flow_mean</li>
<li>AB_packets_per_flow_mean</li>
<li>BA_packets_per_flow_mean</li>
<li>RTT_mean</li>
</ul>
<p>The above mean values are used as QoS features for the ML training and testing sets.<br>
4. Training Set – QoS features from each benchmark instance are matched against the target QoE benchmark instance durations.  A validation set is created in a similar way.<br>
5. Learning – We apply classification ML (both Binary and Multiclass) to infer the measured QoE class from the transformed SkyDive metrics.<br>
6. Model Validation – The model is validated against the validation set.</p>
<h2 id="estimate-workload-duration-qoe-from-measured-qos-features.">Estimate Workload Duration (QoE) from measured QoS features.</h2>
<p>The PoC’s ML evaluation properties are outlined below:</p>
<ul>
<li>Establish threshold boundaries to be used in the classification by examining the QoE measurments</li>
<li>Evaluate for both Binary Classification and Multiclass Classification</li>
<li>Evaluate with all of the ML classifiers</li>
<li>Evaluate with all of combinations of QoS Features</li>
<li>Compare the evaluation methods (classifier and feature combination) and determine the best performers</li>
</ul>
<h2 id="target-qoe">Target QoE</h2>
<p>The target QoE used in the evaluation was the workload duration recorded by the test driver.</p>
<h2 id="qos-features">QoS Features</h2>
<p>The following QoS features were used in the evaluation:</p>
<ul>
<li>flow_duration_mean</li>
<li>bytes_per_flow_mean</li>
<li>packets_per_flow_mean</li>
<li>AB_bytes_per_flow_mean</li>
<li>BA_bytes_per_flow_mean</li>
<li>AB_packets_per_flow_mean</li>
<li>BA_packets_per_flow_mean</li>
<li>RTT_mean</li>
</ul>
<p>All combinations of the above features were exercised to determine which could be used by the classifiers described below to best predict the target QoE.<br>
All of the features were derived from network metrics collected by skydive. Flow duration was derived from the difference between Skydive’s Metric.Last and Metric.Start.</p>
<h2 id="classifiers">Classifiers</h2>
<p>Different scikit classifiers were used in the analysis.  The classifiers were compared to determine which best predicted the target QoE using QoS features described above.<br>
The following classifiers were used in the evaluation:</p>
<ul>
<li>LogisticRegression</li>
<li>DecisionTreeClassifier</li>
<li>KNeighborsClassifier</li>
<li>LinearDiscriminantAnalysis</li>
<li>RandomForestClassifier</li>
<li>GaussianNB</li>
<li>SVC</li>
<li>MLPClassifier  (removed because it was taking too long)</li>
<li>GaussianProcessClassifier(removed because it was taking too long)</li>
<li>AdaBoostClassifier</li>
<li>QuadraticDiscriminantAnalysis</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<p>The QuadraticDiscriminantAnalysis Classifier is the best classifier for both Binary Classification and MultiClass Classification.<br>
The table below summarizes the results:</p>
<ul>
<li>training set 1266</li>
<li>testing set 203</li>
</ul>

<table>
<thead>
<tr>
<th>Classification</th>
<th align="center">Classifier</th>
<th align="center">features</th>
<th align="center">f1_score</th>
<th align="right">accuracy_score</th>
<th align="right">log_loss_score</th>
</tr>
</thead>
<tbody>
<tr>
<td>binary</td>
<td align="center">QuadraticDiscriminantAnalysis</td>
<td align="center">flow_duration_mean<br>bytes_per_flow_mean<br>packets_per_flow_mean<br>BA_bytes_per_flow_mean<br>AB_packets_per_flow_mean<br>RTT_mean</td>
<td align="center">.921</td>
<td align="right">.921</td>
<td align="right">.199</td>
</tr>
<tr>
<td>multiclass</td>
<td align="center">QuadraticDiscriminantAnalysis</td>
<td align="center">flow_duration_mean<br>bytes_per_flow_mean<br>packets_per_flow_mean<br>AB_bytes_per_flow_mean<br>BA_bytes_per_flow_mean<br>BA_packets_per_flow_mean</td>
<td align="center">.833</td>
<td align="right">.833</td>
<td align="right">.415</td>
</tr>
</tbody>
</table><p>This is the python code to create the capture  of the wordpress service:</p>
<pre><code>from skydive.rest.client import RESTClient
import yaml
conf_vars = yaml.load(open('tests_conf.yaml'))
SKYDIVE_IP=conf_vars.get('skydive_ip', '9.148.244.26')
SKYDIVE_PORT=conf_vars.get('skydive_port', '30777')
restclient = RESTClient(SKYDIVE_IP+":"+SKYDIVE_PORT)
restclient.capture_create("G.V().Has('Manager', NE('k8s'),'Docker.Labels.app', Regex('.*wordpress.*'),'Docker.Labels.tier', Regex('frontend')).Both().Out('Name','eth0')")
</code></pre>
<p>This is the python code used to collect the TCP flows resulting from the above capture:</p>
<pre><code>class threadGetSkydiveFlows(threading.Thread):
   def __init__(self):
      threading.Thread.__init__(self)
      self.collectFlows = True
      self._return = (pd.DataFrame(),"")
   def run(self):
      logging.info("thread threadGetSkydiveFlows starttime %d",int(time.time()*1000.0))
      err = ""
      restclient = RESTClient(SKYDIVE_IP+":"+SKYDIVE_PORT)
      gremlinFlow = "G.Flows().Has('Application', 'TCP')"
      flows = restclient.lookup(gremlinFlow)
      dfOldFlows = json_normalize(flows)
      frames = []
      time_out = time.time() + TIME_OUT
      while (self.collectFlows) &amp; (time.time() &lt; time_out):
	    flows = restclient.lookup(gremlinFlow)
	    df = json_normalize(flows)
	    if (not df.empty) &amp; (not dfOldFlows.empty):
	    	cond = df['UUID'].isin(dfOldFlows['UUID']) == True
	    	df.drop(df[cond].index, inplace = True)
	    if not df.empty:
		frames.append(df)
	    sleep(SKYDIVE_SLEEP_TIME)
      if time.time() &gt;= time_out:
	 err = "Error: skydive time out"
	 logging.info(err)
         
      df = pd.DataFrame() 
      if len(frames) &gt; 0:
        df = pd.concat(frames,sort=False)
        df = df.drop_duplicates()
        df = df.sort_values("Metric.Last",ascending=True)
        df = df.drop_duplicates(subset="UUID", keep='last')
      self._return = (df,err)
   def join(self):
      sleep(SKYDIVE_SLEEP) 
      self.collectFlows = False 
      Thread.join(self)
      return self._return
</code></pre>
<blockquote>
<p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>
<p>enter code here</p>

